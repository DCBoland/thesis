\chapter{Adapting Music Retrieval to User Engagement}
\label{chap:eng-dep}
\lettrine{A}{dapting} a music retrieval interaction to the user's desired level of effort and control would enable a single interface to span the range of user engagement. The hypothesis explored in this chapter is that a recommender system could relieve users of the burden of control according to the level of cognitive effort they are willing to invest. Users would be able to seamlessly transition from a casual style of interaction akin to using a radio to more controlling styles such as specifying a particular sub-area of interest in a music space, or even selecting individual songs. An example of such adaptation can be seen in Pandora's recommender system,\footnote{\url{http://www.pandora.com} (11/08/14)} though this interaction is again at a single level of engagement (a simple like or dislike rating of tracks). A demonstrator system is developed, with a mood-based music retrieval that allows users to select a level of control over a recommender system. An agent-based evaluation characterises this interface, and how it can span levels of user engagement, with additional comments elicited from participants in a design session.

\newpage

The tablet-based demonstrator system shows how users can seamlessly take control over recommendation, from hearing any popular music, through to more specific sub-regions of music, all the way to the user explicitly selecting individual items. Users are able to explicitly denote their engagement with a pressure sensor mounted on the tablet's bezel -- employing a metaphor of physically engaging. A large music collection is arranged into a coherent one-dimensional space (ordered by mood), such that broad selections of a mood of music can be made. The system employs a generative model to predict the user's input for each available song. When no control is exerted the expected input is broad and uncertain, with the system using prior evidence (such as song popularity) to make an intelligent inference of which music to recommend. As the user exerts control, the expected inputs become more specific, allowing the system to infer the user's intended song.

There are many sources of evidence of users' engagement context that could be considered for adapting music retrieval.  Music engagement is explored in \chapref{chap:eng}, with measures of user engagement and listening behaviour developed in \chapref{chap:measures}.  While these give some evidence for adapting to the user, there has also been work on measuring engagement in the current listening context, for example by detecting rhythmic movements \citep{KuhWatWir11}. As a proxy for such methods, in order to explore this design space, the system developed in this chapter allows users to denote their engagement explicitly by pressing a pressure sensor -- employing a design metaphor of physical exertion as engagement. The development and evaluation of this sensor is detailed in Appendix \ref{chap:pressuresensing}.

\subsection{Engagement \& Control}
Chapter \ref{chap:eng} has explored user engagement and interactions that bridge from casual to engaged styles of use. If a music retrieval system enabled users to set their level of engagement, it could cover the maximising-satisficing scale -- catering to individual users and their music-listening contexts. This dynamic has important implications for the user's sense of ownership of the music interaction. It is likely that users would no longer feel the sense of pride or embarrassment related to a track selection (as described in \cite{CunJonJon04}) if control was shared with an intelligent music system.

In casual interactions, where the user exerts less control, a system can act more autonomously -- making inferences from prior evidence about what the user intended. This way of handing over control was termed the `H-metaphor' by \cite{FleAdaCon03}, where it was likened to riding a horse -- as the rider loosens the reins, and exerts less control, the horse behaves more autonomously. Music retrieval would benefit from spanning the range of \emph{retrieval control} in this fashion, allowing users to set a level of engagement appropriate to their context. Users would have a corresponding degree of control over the recommendations, ranging from biasing them to a style of music, through to the explicit selection of a particular track.

\subsection{Inferring Listening Intent}
A music retrieval system could adapt to the user's engagement by acting more autonomously when the user wishes to have less \emph{retrieval control}, i.e.\ when they do not have a specific goal in mind. This can be described in terms of the information-theoretic viewpoint discussed in \chapref{chap:eng} -- as the user provides less bits of information to reduce the uncertainty about which song to play, the system must increasingly provide the necessary information. The input from the user can thus be considered coarser as they become less engaged, having less information content and thus less weight on the behaviour of the system, instead indicating the `broad strokes' of the user's intent. Their intended selection can then be inferred using prior evidence of music-listening intent, for example the user's previous listening history, social recommendations or the overall popularity levels of each song.

While the input from the user may be coarser and less precise when the user is less engaged,\footnote{e.g.\ \citeauthor{PohMur13}'s \citeyear{PohMur13} work on casual interaction} in other cases similar input must be evaluated in the context of different engagement levels. For example, a touchscreen event when a user is casually browsing through an overview of a music library should be interpreted more coarsely than an explicit selection of a piece of music. In order to correctly interpret this input, the system must be aware of the engagement context -- either by sensing it (e.g.\ with accelerometers \citep{KuhWatWir11}) or by it being made explicit by the user. The handover of control may be continuous or occur on a number of discrete levels, for example moving from proxemic to touch interaction. This discretisation allows distinct operating modes which are clearly delineated however burdens users with managing these multiple modes. As an alternative, the exploration of engagement in this chapter treats it as a continuous variable, acting as a parameter in the inferential system. This continuity allows for a seamless, consistent and natural interaction.
%\newpage

%<<child-fitts, child='fitts.Rnw'>>=
%@
\newpage
\begin{figure*}[tbph]
\centering
\includegraphics[width=0.95\textwidth]{system}
\caption[Engagement-dependent demonstrator system]{The exemplar system (above) adapts to the user's engagement (their desired level of \emph{retrieval control}), allowing users to explore music at a high-level (e.g.\ selecting a broad mood such as angry music) or seamlessly engage, zooming in to make specific selections (e.g.\ albums in a more specific area). Users retain the ability to fully engage and control the system into an exact album selection.} 
\label{engdep-system}
\end{figure*}

\vspace{-0.25in}
\section{Exemplar System}
\vspace{-0.05in}
This chapter sees the augmentation of a popular music retrieval interface (Spotify\footnote{\url{http://www.spotify.com}}) to explore and demonstrate how music retrieval can be adapted to user engagement. This was developed as a tablet-based prototype, with a pressure sensor attached to the bezel, as in Figure \ref{engdep-system}. Also, a semantic zooming view of a simple linear music space was added, enabling both casual and engaged forms of interaction -- giving users varying degrees of control over the selection of music. As users engage, applying pressure, their input increasingly influences the music recommendation, up to taking complete control. The music was arranged on one axis as this is the simplest -- offering a mode with the least possible engagement (\figref{engdep-out}). By allowing users to make selections from the \textit{general} to the \textit{specific}, the new interface supports both maximising and satisficing and spans levels of engagement, or more specifically, the possible levels of \emph{retrieval control}. Users can make broad and uncertain \textit{general} selections to casually describe what they want to listen to. However, they can also exert more control over the system and force it to play a \textit{specific} album (\figref{engdep-in}). An online overview video of the system and the interaction technique is available.\footnote{\url{http://www.dannyboland.com/mobileHCI15/} (20/02/15)}

\begin{figure}[p]
\centering
\includegraphics[width=0.95\textwidth]{engdep-out}
\caption[Zoomed out, low engagement state of engagement-dependent interface]{The zoomed out (low engagement) state allows users to casually make a selection at any point in the mood space. Such a selection has a high uncertainty with regard to individual albums and so the recommender agent uses popularity as prior belief of listening intent.} 
\label{engdep-out}
\end{figure}

\begin{figure}[p]
\centering
\includegraphics[width=0.95\textwidth]{engdep-in}
\caption[Zoomed in, high engagement state of engagement-dependent interface]{The zoomed in (high engagement) state allows users to make a specific selection of an album, having zoomed in to a specific area of the mood space. Such a selection has a low uncertainty with regard to individual albums and so the recommender agent has negligible influence.} 
\label{engdep-in}
\end{figure}

%\subsection{Modalities}

%\begin{figure}[tb]
%\includegraphics[width=\columnwidth]{pressureFig}
%\caption{A bezel-mounted pressure sensor allows users to assert control.} 
%\label{pressure}
%\end{figure}

%Our system requires two inputs: touchscreen gestures for navigation and selection, and pressure input for engagement. Pressure was selected as a design metaphor of users physically engaging. By applying more pressure, the user exerts more control over the selection, and vice versa. This bimanual interaction technique (as in figure \ref{system}), where touch is used to scroll to and select a song and pressure is used to set engagement, allows users to span a continuous range of input \cite{MclBolBre14}. Guiard's Kinematic Chain Model \cite{Gui87} provides a well-tested characterisation of such bimanual action. The pressure sensor uses an Interlink force-sensitive resistor (FSR) with a transimpedance amplifier as described in the datasheet \cite{FSR}.

% Pinch to zoom would mislead. Explore the physical mapping - pressure vs. precision.

\subsection{Inference}
The handover from casual to engaged interaction relies upon an inferential model of user listening intent. It is assumed that when making casual selections, users will navigate to a broad area of musical interest, e.g.\ happy music. It is also assumed that as users engage and wish to make more specific selections, they will use the richer album art and metadata feedback to navigate to a few (or one) albums of interest. These assumptions about the user's behaviour are encoded as a generative model, i.e.\ one which predicts the user's inputs for a given level of engagement, as in Figure \ref{assertion-variance}. Put simply, it predicts casual users will point roughly near the type of music they want and engaged users will navigate to exactly what they wish to hear. This generative model predicts user input $i_x$ along the $x$ axis conditioned upon a target song and level of engagement. The input is modelled using a Gaussian Mixture Model, with a component corresponding to each song $s_i$ in the music collection (omitting distant songs for scalability). Each component is a Gaussian distribution, with the centroid positioned according to the song's position $x_s$ in the one-dimensional music projection, with distribution width set by the precision $\tau$ parameter:
\begin{equation*}
p\left(i_x \! \mid \! s_i\right)  = \sqrt{\dfrac{\tau}{2\pi}} \, e^{\dfrac{-\tau(i_x - x_s)^2}{2}}.
\end{equation*}
The precision $\tau$ parameter is inversely proportional to the unit variance $\sigma_d^2$ of the distribution. As users apply pressure, the precision is scaled by the denoted engagement $E \in (0,1]$:
\begin{equation*}
\tau = \frac{E}{\sigma_d^2}.
\end{equation*} 
This generative model can be used to infer a belief that a given song $s_i$ is of interest to the user, conditioned upon an input position $i_x$ and the level of user engagement $E$:
\begin{equation*}
p\left( s_i \! \mid \! i_x, E\right) = \frac{p\left(i_x \! \mid \! s_i, E\right) p\left(s_i \! \mid \! E\right)}{p\left(i_x \! \mid \! E\right)}.
\end{equation*}
The prior belief $ p\left(s_i \! \mid \! E\right)$ over the music space allows existing evidence of listening intent to be incorporated, conditioned on the current level of engagement. This work uses the music's current popularity on Spotify so that the system can recommend popular music to the user. Engagement is marginalised out from this prior, making the simplifying assumption that only expected input varies with engagement and not music preference. The system's behaviour is characterised using an agent-based approach, enumerating the possible inputs. As shown in Figure \ref{engagement}, the less engaged the user is, the more the system relies upon its prior evidence -- selecting popular music in the broad area navigated to. At higher levels of engagement, the system defers to the user's input, selecting nearby songs except those it considers unlikely.

\afterpage{%
    \clearpage% Flush earlier floats (otherwise order might not be correct)
    %\thispagestyle{empty}% empty page style (?)
    \begin{landscape}% Landscape page
    \begin{figure*}[p]
    \centering
    \includegraphics[width=0.95\linewidth]{assertion_graphs_small}
    \caption[Adapting Music Retrieval to user's exerted engagement]{As the user exerts control, the distribution of predicted input for a given song becomes narrower. This means the system can give more weight to the user's input and infer a belief about their listening intent over fewer songs. (a) shows a casual music selection, with no pressure applied, the user sees the entire axis with mood labels to give a sense of the layout. (b) shows a more engaged music selection, with the user now applying pressure, the view zooms semantically to show album artwork for a range of music of a particular mood, allowing users to get more specific recommendations of popular music. (c) shows a fully engaged music selection, when maximum pressure is applied, users are able to zoom in to view a specific album artwork and make an exact selection, with the system deferring entirely to their input.} 
    \label{assertion-variance}
    \end{figure*}
    \end{landscape}
    \clearpage% Flush page
}

\subsection{Emergent Behaviour}

When the user has not applied pressure, they have a low level of engagement in the retrieval. The view of the music space is thus zoomed out, with mood labels describing its layout (Figure \ref{assertion-variance}a). The inferred music selections are broad, covering an entire region of their collection and are biased by prior evidence (popular tracks). At low levels of engagement it is likely that most tracks played would be highly popular tracks. This behaviour is a design assumption, users may want the system to use other prior evidence such as recency of album release. 

When the user applies pressure, the system interprets this as the user taking control to make a more exact selection. The inferred selection is more specific, and the view zooms semantically to show the album art of the selection (Figure \ref{assertion-variance}b). This selection is a combination of evidence from the user's navigated position with prior evidence, i.e.\ song popularity. Users retain the ability to make explicit selections by fully applying pressure (Figure \ref{assertion-variance}c). By varying the pressure, users seamlessly set their level of engagement and control over music recommendation and selection.

\subsection{Conditional Dynamics \& Semantic Zooming}
The level of detail presented to the user should be appropriate for their level of engagement. This is a natural application for semantic zooming -- users making broad selections would not benefit from seeing the artwork of each album and so the view zooms out, showing only general labels such as a mood. This approach can be generalised, to condition other aspects of the interaction upon the engagement and inference of user listening intent. Not only can  songs be played that are considered of interest to the user but the mappings of the input and output modalities could vary according to the engagement and inferred selection. An example would be adjusting the semantic zooming to fit the posterior (inferred) distribution, ensuring all songs of interest are displayed. An appealing benefit of such conditional mapping functions is that the nature of the inference is exposed to the user. When swiping through the music space, users could find the navigation attracted to or repelled by songs according to popularity. This stickiness could also occur in zooming, with extra pressure required to zoom into unpopular tracks. Such sticky interface elements echo past work in HCI such as \cite{CocFir04}, where they were shown to be beneficial to target acquisition and popular with users. The greatest disadvantage to generative modelling is that where the model does not conform well to the user, the user will be unable to predict the behaviour of the system. As the interaction is increasingly conditioned upon the inference, the exposure to this problem is greater. 

\cite{ShnPla04} recommend adhering to the principle of `direct manipulation' such that the mapping between input and output is clear to the user. In this work, a conservative approach is taken, engagement is mapped linearly with applied pressure and the semantic zooming is done linearly with engagement. Only the graphical feedback is manipulated to display the posterior distribution. Tracks are sampled from the posterior in real-time and their album artwork is highlighted with a faint glow. Tracks are also shown in the playlist frame of the Spotify UI in order of inferred listening intent.


\subsection{A Scalable Music Space}
%Show that people use mood, 1D for casual, note that higher dimensions could allow even further engagement and exploration.

The simplest possible representation of a music space is a one dimensional projection. An assumption of the system is that similar songs are placed near to each other, such that when users are disengaged, they can select a whole coherent region of the music space. Information about the mood of the music is used to arrange the collection however genre or other features and metadata could be used. A high dimensional music feature space was produced using MoodAgent,\footnote{\url{http://www.moodagent.com/} (11/08/14)} a commercial music signal processing system focused on mood-related music features. While a single one of these features (e.g.\ tempo) could be used to arrange the music collection along one dimension, this work attempts to maintain more of the information by using non-linear dimensionality reduction. 

\cite{VenPelNyb10} introduce an information retrieval perspective on such projection techniques, presenting their state-of-the-art NeRV algorithm for balancing recall and precision in the projection. This trade-off between keeping similar items together and dissimilar items apart can be modified using a $\lambda$ parameter. NeRV is used here to project the high dimensional music feature space into a one dimensional space -- \figref{mood-projection} shows how the original features influence this projection. In this example, anger is inversely correlated with tenderness, with joyful music peaking in the centre of the projection. 

The resulting one dimensional projection broadly reflects the culturally universal arousal aspect of mood \citep{EgeFerChu15}, spanning from quiet, calm music to faster, aggressive music. The use of a single dimension is justified by the observation by \citeauthor{EgeFerChu15} that the valence dimension of mood is subjective. The relatively denotative arousal aspect of mood is more readily captured by machine listening methods. The use of mood itself for music retrieval is supported by evidence that mood features describe music as well as genre \citep{BolMur14} and that users frequently listen to music according to mood \citep{LonNor11}. 

%An assumption of our system is that similar songs are placed near to each other, such that when users are disengaged, they can select a whole coherent region of the music space. We take a high dimensional feature space produced by MoodAgent,\footnotemark[5] a commercial music signal processing system focused on mood-related music features. While we could use a single one of these features (e.g.\ tempo) to arrange the music collection, we attempt to maintain more of the information by using non-linear dimensionality reduction. Venna et al.\ introduce an information retrieval perspective on such techniques, presenting their state-of-the-art NeRV algorithm for balancing recall and precision in the projection \cite{VenPel10}. Using NeRV we project the high dimensional music feature space into a one dimensional space -- we show how the original features influence this projection in Figure \ref{mood-projection}. In this example, anger is inversely correlated with tenderness, with joyful music peaking in the centre of the projection.

\clearpage

\begin{figure}[!t]
%\centering
%\vspace{-0.05in} 
\includegraphics[width=0.8\textwidth]{mood-projection}
\caption[Low dimensional projection of a music feature space]{The music is arranged by taking the mood features such as Anger, Joy and Tender and creating a one-dimensional projection. Users are then able to make broad selections from a mood region, as well as pick individual albums.}
\label{mood-projection}
\vspace{-0.2in}
\end{figure}

\begin{figure}[!b]
\vspace{-0.2in}
\hspace{0.12in}\includegraphics[width=\textwidth]{engagement}
\caption[Agent-based characterisation of the engagement-dependent system]{Agent-based characterisation of the required accuracy for song selection against user engagement. Popular songs can be selected by casually navigating near them. Unpopular songs are only selected when the user is highly engaged and navigates exactly to them. As the user applies pressure, denoting increased engagement and their wanting greater \emph{retrieval control}, they take control from the recommender system. As the user takes control, the songs become equally targetable, with the popularity-based prior having diminished influence.} 
\label{engagement}
\end{figure}

\section{Evaluation \& Discussion}
It is inherently difficult to find an objective means of evaluating such a personalised music system. \cite{SchFle12} identify this challenge as a key reason that little research work has been done in personalised music systems. They note the need to consider the user from an early point in the development process. The use of generative modelling is an example of such a user-centred development approach. It allows the simulation of user input to characterise the behaviour of the system across the space of possible input, as in \figref{engagement}. A series of exploratory design sessions were also conducted, to obtain qualitative feedback about the system. The music space used was taken from the UK Top 40 singles charts over the past 50 years. Audio was streamed from Spotify, with the engagement-dependent interface integrated within the Spotify web player UI.

%The generative modelling approach used in this work centres the entire development process around the user -- a good model should predict user behaviour, providing a natural evaluative methodology. We take advantage of the fact that the system is built upon a generative model by comparing the model's predictions against actual user behaviour. After familiarising users with the system and its behaviour, we disable the inferential behaviour and ask users to provide inputs for a given interaction scenario. Using the generative model we are able to calculate the likelihood of these inputs and in doing so, gain a measure for how well the generative model fits user behaviour. Optimising the likelihood of the user input for specific users would allow us to adapt the interaction to individual differences. Alternatively we can optimise across users as part of a user centered design process. 

\subsection{Engagement-Dependent Retrieval}
As the system is based upon a generative model of user input, it can be used to characterise the input required from users in various scenarios. Using the design assumption of users casually choosing popular music, it can be shown how input required from users varies with the popularity of the retrieved track. Figure \ref{engagement} shows how accurately the user must navigate to a song for it to be played, at different levels of engagement and for songs of different popularity. The behaviour of the system can thus be linked to the music listening profiles identified as targets in \secref{sec:profiles}.

\begin{description}
\item[Casual] This profile called for allowing users to satisfice and make corrective actions with little effort. The example system supports this with recommendations of popular music from the broad area navigated to, requiring only swipe gestures to select or change music. The recommender system largely controls the music selection, with coarse high-level influence from user input. Notably, it is impossible to select the least popular songs at low engagement.
\item[Engaged] This profile called for allowing users to invest effort in making an exact selection of an album. The system supports this by letting users apply pressure to zoom into a specific mood region, or even an album of interest. At high levels of engagement, the user input dwarfs the recommender's influence, with popularity having little effect.  
\item[Mixed] The intermediate levels of engagement are supported by letting users quickly set their engagement, with the continuous handover of control using the pressure sensor. As users engage, their input has increasing influence. Semantic zooming gives users feedback about how specific a query at a given level of engagement would be.
\end{description}

\subsection{Design Sessions}
Six participants, in groups of 2, took part in informal design sessions which focused on discussing the music interaction described in this work, and in particular the benefits of being able to engage with a mood-based, semantically zooming view. Each was given time to freely explore the collection and to experiment with making a variety of music selections at different levels of engagement. This experience was used to generate discussion with each of them about how they currently find and listen to music and how the system might support or enhance those habits. As mentioned in the Collaborations statement, the experimental system was given to Ross McLachlan who then conducted and annotated the discussions with users, which are explored here.

\subsubsection{Shuffle and Casual Music Selection}
Five of the participants reported using both shuffle and explicit selection to play music in their regular listening, while only one exclusively used explicit selection. Participants stated that they typically use shuffle for background music or alleviating the social pressures of selecting music, as discussed in \secref{sec:toomuchchoice}. This supports the prediction that the ability to vary the degree of engagement is desirable, with four participants often creating playlists of songs specifically to shuffle over. These playlists were based on personal heuristics or `idiosyncratic genres' as described by \cite{CunJonJon04}. They were often kept to a small number of songs to guarantee similarity between items. It is worth highlighting that users are seeking to engage with their music system at a level between shuffle and explicit selection, which is largely unsupported in existing retrieval interfaces.

\subsubsection{Curation and Taking Control}
Participants all expressed a desire to curate the collection. One suggested that there could be more room for subtle differences between tracks in a personal collection, which would allow finer grained control over even uncertain selections. One pair perceived the underlying structure of the music space to be close to random and felt that having more control over the music in the collection would lead to a greater understanding of the organisation, commenting that ``I'd like to be able to apply more of my own decision making.'' This sentiment was echoed in comments about the labels. One participant commented that self-defined labels would be better, even if not fully representative of the underlying mood -- ``people might be annoyed if all their favourite albums are classed as `sad'... I would prefer `melancholic', it has nicer connotations.'' Clearly, a desire for customisation is present that could be accommodated for by allowing users to define the underlying collections, as opposed to generating it from the Top 40 charts. 

\subsubsection{Selection Feedback}
The visual feedback used for low engagement (mood labels on the axis) misled users into assuming that the music was classified into distinct mood categories. Users did not perceive the probabilistic nature of the mood space. For example, the point where `Joy' ended and `Anger' began was a constant topic of discussion. There is an inherent uncertainty to the music layout and selection that was not conveyed. More abstract feedback would have better communicated the nature of the selection, and that each song is a mixture of moods. For example colouring the music space according to mood, with a blending of colours as mood overlaps, however any extra complexity would have required more user training and reduced the system's ability to support the most casual retrieval scenarios.

Participants found the semantic zooming to be useful, but felt that the granularity should increase more evenly. Making very broad or very precise selections was easier than making intermediate selections. The visual feedback presented in intermediate positions, the line of very small album artworks, ``invites you to zoom in'' rather than providing feedback about what selection will happen at that zoom level. Participants wanted even finer control over their level of engagement in their music retrieval, which lends support to the argument for engagement-dependent music retrieval.

% Modalities - pressure - range of ways to use it.
 
%\subsection{Shared Control Radio Interaction}

%The experimental task asking users to recreate predicted input for a fixed selection provided an insight into how well the assumptions made in the generative model fit users. The linearity of predicted input against actual input indicates that our assumed mapping function between pressure and variance is largely correct. We are able to use the results as part of an iterative design process to improve the generative model, e.g.\ users expect variance to reduce more rapdily as they assert pressure. We note that at high levels of pressure (users asserting full control), user input was subject to higher variance and often not as predicted, instead plateauing below full assertion. This result suggests users stopped increasing pressure despite not yet reaching full assertion, perhaps due to the visual zooming feedback suggesting they had `zoomed enough'. Using the conditional mapping functions described earlier to condition zooming on the posterior distribution would counteract this confounder. Relatedly, because of the visual feedback, it is unsurprising that users were able to easily position the dial as predicted. The posterior distribution will be roughly centred upon the target dial position -- the participant thus simply had to centre the visualised posterior to achieve a good result. We see that whilst our generative model's assumptions are valid, they can be tweaked to user behaviour as part of an iterative design process. We also note that using the observed inter-user differences, we could adapt the interaction to specific user behaviour.

\vspace{-0.1in}
\subsubsection{Sources of Evidence}
The present implementation of the system uses music popularity as a source of evidence for weighting songs as being more likely to be played. This comes from a design assumption that satisficing is supported by recommendations based on a prior such as popularity. The design session revealed that users felt that in doing this, recent additions to their music collection would be underplayed. By including when the album was added to the collection as prior evidence, the selection can be biased towards recent additions as well and this use case can be supported. A wide range of other sources of evidence could be used in place of popularity, such as the user's music listening history or, from the commercial perspective, the artists currently being promoted by music labels.  

\vspace{-0.1in}
\subsection{Music Spaces}
The behaviour of this system is closely linked to the music space used. In this work the music space used is likely to be familiar to all participants, however this does introduce some issues. In early testing the music space comprised the most popular 5000 tracks from the UK charts over the last five decades, it was found that the resulting music space was too homogeneous to navigate meaningfully. In an effort to increase the diversity of the music space, tracks were randomly sampled from the music collection instead, yielding the distinct mood regions seen.

Users disliked when dissimilar songs were together. A benefit of using the NeRV algorithm to project the music arrangement is being able to adjust the $\lambda$ parameter to favour precision (keeping dissimilar items apart) over recall (keeping similar items together). Using feedback from users while iterating the design of the system, $\lambda$ was biased strongly toward precision to keep dissimilar items apart and improve perceived system quality. The need to avoid stark outliers has been noted within MIR; Paul Lamere of Echonest introduced the `WTF test' for automatic playlists, with systems scored negatively for each outlier.\footnote{\url{http://musicmachinery.com/2011/05/14/how-good-is-googles-instant-mix/} (11/04/14)} %The true test for an interface such as the one presented would be to use the user's own music collection for the projection, as part of a longitudinal study of music usage.

%\subsection{Wider Application}
%This engagement-dependent approach has applications beyond music retrieval, with users now able to access large volumes of media such as movies and photos, e.g.\ with Netflix and Flickr. We argue that users are better served by letting them engage in a retrieval which is appropriate to their context. While recommender systems have done much in the way of alleviating users of the issue of too-much-choice, there will always be cases where users wish to have more input into the selection. Incorporating user engagement into recommender systems can empower users to benefit from recommendation that suits their current context.

\section{Conclusions}
Having motivated the need for music retrieval that adapts to user engagement throughout this thesis, this chapter shows one way in which this may be achieved. The exemplar system serves to illustrate not only the benefits of this adaptation, but also some of the inherent limitations. Allowing users to seamlessly change their engagement, from scanning broad labels to specific artwork, supports distinct use cases. It is less clear, however, how to support intermediate states of engagement. Additional levels of feedback can be added, such as labelling landmark tracks or artists, or displaying other metadata. There is an opportunity for further work on providing overviews of music collections, in particular with semantic zooming. The use of audio feedback, for example by mixing samples from the inferred track selections, is one such approach.

\subsection{Mood}

Information about the mood of the music is used to arrange the collection, however, genre or other features and metadata could be used instead. The choice of mood for use in this work is supported by studies of why users listen to music, such as \cite{LonNor11}. The music features adopted are widely used, having been acquired from a popular commercial service. They provide a coherent music space however this is only to serve as an illustration. Reliably detecting mood and genre from audio samples is an ongoing area of research, with a number of challenges remaining (see \chapref{chap:mir} and \cite{Stu14}). 

Reducing the music features to one dimension provided the simplest music space for users to navigate, at the lowest levels of engagement. The more complex the music space presented to the user, the more time would be required for familiarisation with the space. Future work would benefit, however, from considering a music space with higher dimensionality, as this would support an investigation into more highly engaged exploration.

%In design sessions, users were able to casually nudge the system towards a broad area of a music collection, with the system inferring relevant songs from this and prior evidence such as song popularity. Alternatively, users could choose to engage, zooming in semantically to provide more evidence of their listening intent. Feedback from users described how our system supported their music-listening habits, calling for even greater control over their level of engagement. By adapting to how much effort users want to invest in their music retrieval, we enabled access to recommendations from an entire Big Music collection whilst retaining users' ability to take control. Users were thus able to make selections as casually or specifically as their mood dictated.

\subsection{Market-Based Evaluation}
While the behaviour of the system has been demonstrated using technical measures and qualitative feedback, it would be desirable to study the longitudinal impact that using such a system would have on listening behaviour. At this point, rather than conducting further small-scale studies, it is preferable to widely deploy a full music system, which users can adopt for their day to day listening. The concepts presented in this work have been applied in a commercial, tablet format, engagement-dependent music retrieval system -- the BeoSound Moment, announced at CES 2015. This product is part of an ongoing commercial evaluation, with some initial evaluation efforts described in \chapref{chap:industrial}.

%This work motivates and illustrates how retrieval can be adapted to engagement, balancing control between the user and a recommender system. Users are currently forced into having full or little control over their music retrieval. We show that adapting retrieval to engagement supports use cases from casual radio-like recommendation to selecting sub-categories of music or even specific item retrieval.