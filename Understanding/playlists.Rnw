\section{User-Centred Feature Selection}
\label{sec:userfeatures}

<<playlists-setup, cache=TRUE>>=
library(infotheo)
library(reshape)
library(ggplot2)
library(changepoint)
library(zoo)
library(RColorBrewer)
library(RSQLite)

getAMI <- function(feature, class) {
  I <- mutinformation(class, feature)
	n0 <- mutinformation(class[sample.int(length(class))], feature)
	#return((I - n0) / (max(entropy(class), entropy(feature)) - n0))
	return((I - n0) / (entropy(feature) - n0))
}

# None of this is reproducible, uses confidential data
con <- dbConnect(SQLite(), "C:/syntonetic/bigmusic.db")
data <- dbGetQuery(con, "SELECT * FROM [entropy];")

# Skip broken users
data <- subset(data, user!=1354&user!=2724&user!=151)

# Tidy up and discretize
colnames(data)[which(colnames(data)=="tempo feel")] <- "tempo"
features <- data[,6:(ncol(data)-4)]
features$popularity <- discretize(data$popularity,nbin=7)$X #200)$X
features$duration <- discretize(data$duration,nbin=7)$X
class <- data$playlist

# Get AMI between features and playlistID (class)
fami <- apply(features,2,getAMI,class)
@

Identifying which music features best describe a range of playlists is not only useful for playlist recommendation, but also provides an insight into how users organise and think about music. Music recommendation and playlist generation typically work on the basis of genre, mood and popularity, and this chapter investigates which of these music features reflect the differences between playlists created by users. 

As the existing music retrieval systems used by listeners are based upon these features, it is likely that a `chicken-and-egg' effect will apply, where the features which best describe user playlists are those which users are currently exposed to in their existing retrieval interfaces.

\subsection{Mutual Information}
Information-theoretic measures can be used to identify to what degree a given feature shares information with class labels. For a feature $X$ and a class label $Y$, the mutual information $I \! \left(X ; Y\right)$ between these two can be given as:
\begin{equation}
I \! \left(X ; Y\right) = H \! \left(X\right) - H \! \left(X \! \mid \! Y\right),
\end{equation}
that is, the entropy of the feature $H \! \left(X\right)$ minus the entropy of that feature if the class is known $H \! \left(X \! \mid \! Y\right)$. Taking membership of playlists as a class label, it is then possible to determine how much can be known about a song's features if one knows what playlist the song is in. When using mutual information to compare clusterings in this way, care must be taken to account for random chance mutual information \citep{VinEppBai10}. This approach is adapted to focus on how much the feature entropy is reduced, and is normalised against the expectation of this feature entropy accordingly:
\begin{equation}
AMI \! \left(X; Y\right) = \frac{I \! \left(X ; Y\right) - E \! \left[I \! \left(X ; Y\right)\right]}{H \! \left(X\right) - E \! \left[I \! \left(X ; Y\right)\right]},
\end{equation}
where $AMI \! \left(X; Y\right)$ is the adjusted mutual information and $E \! \left[I \! \left(X ; Y\right)\right]$ is the expectation of the mutual information, i.e.\ due to random chance. The AMI gives a normalised measure of how much of the feature's entropy is explained by the playlist. When $AMI = 1$, the value of the given feature for each track is known exactly if the playlist is known. When $AMI = 0$, nothing about the feature is known if the playlist is known. Those features that have a high AMI with playlist membership are related to the way in which user's organise their music, and are thus of greater interest when designing and evaluating music retrieval systems.
\newpage 

\subsection{Linking Features to Playlists}
<<playlists-featuresplot, eval=TRUE, dev='png', fig.scap="Mutual Information between playlists and music features", fig.cap="Features are ranked by their Adjusted Mutual Information with playlist membership. Playlists are distinguished more by whether they contain \\textsc{rock} or \\textsc{angry} music.", fig.pos='!b', fig.height=5.25, fig.width=7, dependson=c('playlists-setup')>>=
fs <- c("rock","angry","electronica","tender","rnbsoul","popularity","sad","world","duration","joy","jazz","gospel","tempo")
bestFeatures <- data.frame(feature=names(fami[fs]),MI=fami[fs])
#bestFeatures <- data.frame(feature=names(fami),MI=fami)
bestFeatures <- bestFeatures[order(bestFeatures$MI, decreasing=TRUE),]
bestFeatures$feature <- factor(bestFeatures$feature, levels=bestFeatures$feature)
bestFeatures$MI <- bestFeatures$MI * 100
ggplot(bestFeatures, aes(x=feature, y=MI, fill=MI)) + 
  geom_bar(stat="identity") + scale_fill_continuous() + #h=c(360,180)) + 
  guides(fill=FALSE) + xlab("Feature") + ylab("% of uncertainty explained by playlist") +
	#ggtitle("Mutual Information Between Playlists and Features") +  
	theme_bw(base_size = 12, base_family = "Helvetica") + theme(panel.border = element_blank(), panel.grid=element_blank()) +
	theme(axis.text.x = element_text(angle = 90, hjust=1, vjust=0.25), axis.text.y = element_text(hjust=1))
@

The \Sexpr{prettyPrint(playlist_count$COUNT)} playlists in the \textit{SPUD} dataset were used to calculate their AMI with a variety of high level music features from Syntonetic and Spotify. The ranking of some of these features is given in \figref{fig:playlists-featuresplot}. The aim here is merely to illustrate this approach, as any results are only as reliable as the underlying features. With this in mind, the features \textsc{rock} and \textsc{angry} had the most uncertainty explained by playlist membership. While the values may seem small, they are calculated over many playlists, which may combine moods, genres and other criteria. As the features with a high AMI change most between playlists (rather than within them), they are the most useful for characterising the differences between the playlists. The \textsc{duration} feature ranked higher than expected, further investigation revealed playlists that combined lengthy DJ mixes. It is perhaps unsurprising that playlists were not well characterised by whether they included features such as \textsc{world} music. It is of interest that \textsc{tempo} was not one of the highest ranked features, illustrating the style of insights available when using this approach. Further investigation is required to determine whether playlists are not based on tempo as much as is often assumed or if this result is due to the peculiarities of the proprietary perceptual tempo detection.

\subsection{Feature Selection}
Features can be selected using information-theoretic measures, with a thorough treatment of this field given by \cite{BroPocZha12}. They define a unifying framework within which to discuss methods for selecting a subset of features using mutual information. This is done by defining a J criterion for a feature $f_n$ and (playlist) class $C$ given the previously selected feature subset $S$:
\begin{equation}
J\left( f_n \right) = I \! \left(f_n ; C \mid S\right). % = H \! \left(X\right) - H \! \left(X \! \mid \! Y\right)
\end{equation}
This gives a measure of how much information the feature shares with playlists given some previously selected features, and can be used as a greedy feature selection algorithm. Intuitively, features should be selected that are relevant to the classes but that are also not redundant with regard to previously selected features. A range of estimators for $I \! \left(f_n ; C \mid S\right)$ are discussed in \cite{BroPocZha12}.

As a demonstration of the feature selection approach described, it is applied to the features depicted in \figref{fig:playlists-featuresplot}, selecting features to minimise redundancy. The selected subset of features in rank order is: \textsc{rock}, \textsc{duration}, \textsc{popularity}, \textsc{tender} and \textsc{joy}. It is notable that \textsc{angry} had an AMI that was almost the same as \textsc{rock}, but it is redundant if \textsc{rock} is included. Unsurprisingly, the second feature selected is from a different source from the first -- the duration information from Spotify adds to that used to produce the Syntonetic mood and genre features. Reducing redundancy in the selected features in this way yields a very different ordering, though one that may give a clearer insight into the factors behind users'  construction of playlists.

\section{Discussion}
The feature selection shown in this chapter is done directly from the user data. In contrast, feature selection is usually performed using classifier wrappers with ground truth class labels such as genre. The use of genre is based on the assumption that it would support the way users currently organise music and features are selected based on these labels. This has lead to issues including classifiers being trained on factors that are confounded with these labels and that are not of relevance to genre or users \citep{Stu14}. The user-centred approach introduced here selects features independently of the choice of classifier, in what is termed a `filter' approach. The benefit of doing this is that a wide range of features can be quickly filtered at relatively little computational expense. While the classifier `wrapper' approach may achieve greater classifier performance, it is more computationally expensive and more likely to suffer from overfitting (as common in genre classification for example). 
\newpage

The key benefit of filtering features based on user behaviour is that it provides a perspective on music features that is free from assumptions about users and music ground truth. This user-centred perspective provides a sanity-check for music features and classification -- if a feature does not reflect the ways in which users organise their music, then how useful is it for music retrieval?

\subsection{When To Learn}
The information-theoretic measures presented offer an implicit relevance feedback for users' music retrieval. While this chapter has considered the entropy of features as reflecting user behaviour, this behaviour is conditioned upon the existing music retrieval interfaces being used. For example, after issuing a query and receiving results, the user selects relevant songs from those results. If the entropy of a feature for the songs selected by the user is small relative to the full result set, then this feature is implicitly relevant to the retrieval. 

The identification of shuffle and explorative behaviour provides some additional context for this implicit relevance feedback. Music which is listened to in a seemingly random fashion may represent an absent or disengaged user, adding noise to attempts to weight recommender systems or build a user profile. At the very least, where entropy is high across all features, then those features do not reflect the mental model currently being employed by the user for their music retrieval. The detection of shuffle or high-entropy listening states thus provides a useful data hygiene measure when interpreting listening data. 

\subsection{Engagement}
The entropy measures capture how much each feature is being `controlled' by the user when selecting their music. It has been shown that it spans a scale from a user choosing to listen to something specific to the user yielding control to radio or shuffle. Considering entropy over many features in this way gives a high-dimensional vector representing the user's engagement with music. Different styles of music retrieval occupy different points in this space, commonly the two extremes of listening to a specific album or just shuffling. There is an opportunity for music retrieval that has the flexibility to support users engaging and applying control over music features only insofar as they desire to. An example of this would be a shuffle mode that allowed users to bias it to varying degrees, or to some extent, the feedback mechanism in recommender systems. This information-theoretic characterisation of listening behaviour and user control provides further quantitative grounding to the discussion of engagement and control in \secref{sec:musicengagement}.